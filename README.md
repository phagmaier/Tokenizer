# ğŸ”¡ Tokenizer

[![License](https://img.shields.io/github/license/phagmaier/Tokenizer)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/phagmaier/Tokenizer?style=social)](https://github.com/phagmaier/Tokenizer/stargazers)
[![Last Commit](https://img.shields.io/github/last-commit/phagmaier/Tokenizer)](https://github.com/phagmaier/Tokenizer/commits/main)
[![Lines of Code](https://img.shields.io/tokei/lines/github/phagmaier/Tokenizer)](https://github.com/phagmaier/Tokenizer)
[![Language](https://img.shields.io/badge/language-C-blue.svg)](https://en.wikipedia.org/wiki/C_(programming_language))
[![Repo Size](https://img.shields.io/github/repo-size/phagmaier/Tokenizer)](https://github.com/phagmaier/Tokenizer)

A high-performance tokenizer written in C, built for **blazing-fast** text processing. It uses **multithreading** and a **custom memory pool** to tokenize large files with minimal overhead. Tokenization is performed using the **Byte Pair Encoding (BPE)** algorithm.

---

## âš ï¸ Current Status

- âœ… Vocabulary generation works  
- âš ï¸ Functions for reading vocab and tokenizing new text exist but are **not yet tested**
- âš ï¸ If data is sparse or repetitive in some areas or if token size is too large for text the program may run forever **NEED TO FIX THIS**

---

## âœ¨ Features

- ğŸ”€ **Multithreaded processing** â€” Parallel tokenization using POSIX threads  
- ğŸ§  **Custom memory pool** â€” Fast and controlled memory allocation  
- ğŸ§© **Byte Pair Encoding (BPE)** â€” Efficient subword segmentation  
- ğŸ§® **Configurable vocabulary size**  
- ğŸ“¦ **Adjustable chunk size per thread**

---

## ğŸ› ï¸ Tech Stack

- **C** â€” Core implementation  
- **CMake** â€” Build system  
- **pthreads** â€” Concurrency

---

## ğŸ“‹ Requirements

- ğŸ§° C compiler (GCC/Clang) with **C23 support**  
- âš™ï¸ CMake â‰¥ 3.16  
- ğŸ› ï¸ Make (usually bundled with Unix-like systems)

---

## âš™ï¸ Build Instructions

```bash
# Clone the repository
git clone https://github.com/phagmaier/Tokenizer.git

# Enter the project directory
cd Tokenizer

# Create and move into the build directory
mkdir build && cd build

# Configure and build
cmake ..
cmake --build .
```

For a release build:

```bash
cmake --build . --config Release
```

âœ… `runme` will be available in the `build` directory.

---

## ğŸš€ Usage

### CLI Flags

| Flag | Description           | Default              |
|------|-----------------------|----------------------|
| `-i` | Input file path       | `../data/data.txt`   |
| `-o` | Output file path      | `../data/myTokens.txt` |
| `-v` | Vocabulary size       | `10000`              |
| `-b` | Bytes per thread      | `250000`             |
| `-t` | Max threads           | `8`                  |

**Output Format:**

```
<token_id>,<token_string>\n
```

### Example

Custom run:

```bash
./runme -i ../data/infiniteJest.txt -v 10000 -b 500000 -o ../data/myTokens.txt -t 8
```

Default run:

```bash
./runme
```

âš ï¸ File paths should be relative to `build/` or be absolute.

---

## ğŸ—‚ï¸ Project Structure

```
Tokenizer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Dics.c/.h         # Dictionary & hash table
â”‚   â”œâ”€â”€ Helper.c/.h       # Utility helpers
â”‚   â”œâ”€â”€ Token.c/.h        # Token data structures
â”‚   â””â”€â”€ Tokenizer.c/.h    # Core tokenizer logic
â”œâ”€â”€ CMakeLists.txt        # Build config
â””â”€â”€ data/                 # Sample input/output files
```

---

## âš¡ Benchmarks

- ğŸ“˜ *Infinite Jest* (3.2MB) â€” ~11 sec with 8 threads (default settings)  
- ğŸ“¦ 167MB corpus â€” ~15 sec with 15 threads, 50kB chunks

ğŸ’¡ Increase chunk size for better accuracy. Decrease it for more speed.

---

## ğŸ§  Limitations

1. âŒ Only supports ASCII  
2. âœ‚ï¸ Chunk splitting may occur mid-word (needs smarter segmentation)

---

## âœ… TODO

- [ ] Prevent chunk splits mid-word  
- [ ] Test vocab-based tokenization  
- [ ] (Nice-to-have) Add NN/Transformer training integration
- [ ] Find a nice way to exit early and or try other chunk sections if chunk cannot generate more vocab

---

## ğŸ¤ Contributing

No formal contributing guide yet â€” but PRs are welcome!

---

## ğŸ“« Contact

**Parker Hagmaier**  
ğŸ“§ [parkerhagmaier@gmail.com](mailto:parkerhagmaier@gmail.com)

```

